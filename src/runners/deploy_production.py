#!/usr/bin/env python3
"""
Production deployment validation for Runner System.

Validates the complete production runner system including:
- Metadata file availability and validation
- Runner initialization from JSON files
- Inference execution with error handling
- Security calculation and verification
- Time/size limit enforcement
- Output generation and file overwriting
"""

import os
import sys
import json
import time
import logging
from typing import Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def validate_system() -> bool:
    """
    Comprehensive system validation for production deployment.
    
    Returns:
        True if all validations pass, False otherwise
    """
    print("üîç PRODUCTION RUNNER SYSTEM VALIDATION")
    print("=" * 60)
    
    success_count = 0
    total_tests = 0
    
    try:
        # Test 1: Import validation
        total_tests += 1
        print(f"\nüß™ Test 1: Import Validation")
        try:
            from runner_metadata import RunnerMetadata
            from runner import Runner
            print("‚úÖ All imports successful")
            success_count += 1
        except ImportError as e:
            print(f"‚ùå Import failed: {e}")
            return False
        
        # Test 2: Model availability
        total_tests += 1
        print(f"\nüß™ Test 2: Model Availability")
        models = ["models/doom", "models/net"]
        available_models = []
        
        for model_path in models:
            metadata_file = f"{model_path}/{os.path.basename(model_path)}_Runner_Metadata.json"
            if os.path.exists(metadata_file):
                available_models.append(model_path)
                print(f"‚úÖ {model_path}: Metadata available")
            else:
                print(f"‚ö†Ô∏è  {model_path}: Metadata missing - will test generation")
        
        if available_models:
            success_count += 1
        
        # Test 3: Metadata file validation
        total_tests += 1
        print(f"\nüß™ Test 3: Metadata File Validation")
        
        for model_path in available_models:
            try:
                metadata_file = f"{model_path}/{os.path.basename(model_path)}_Runner_Metadata.json"
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                # Validate required fields
                required_fields = [
                    "model_name", "overall_security", "execution_chain", 
                    "verified_slices", "slices", "io_paths", "time_limit", "size_limit"
                ]
                
                missing_fields = [field for field in required_fields if field not in metadata]
                if missing_fields:
                    print(f"‚ùå {model_path}: Missing fields: {missing_fields}")
                else:
                    print(f"‚úÖ {model_path}: Metadata structure valid")
                    
            except Exception as e:
                print(f"‚ùå {model_path}: Metadata validation failed: {e}")
                
        success_count += 1
        
        # Test 4: Runner initialization from JSON
        total_tests += 1
        print(f"\nüß™ Test 4: Runner Initialization from JSON")
        
        runners = {}
        for model_path in available_models:
            try:
                runner = Runner(model_path)
                runners[model_path] = runner
                
                # Validate runner properties
                security = runner.overall_security
                time_limit = runner.time_limit
                size_limit = runner.size_limit
                
                print(f"‚úÖ {model_path}: Runner initialized")
                print(f"   Security: {security}% (whole number)")
                print(f"   Time limit: {time_limit}s")
                print(f"   Size limit: {size_limit/1024/1024:.1f}MB")
                
            except Exception as e:
                print(f"‚ùå {model_path}: Runner initialization failed: {e}")
                
        if runners:
            success_count += 1
        
        # Test 5: Inference execution
        total_tests += 1
        print(f"\nüß™ Test 5: Inference Execution")
        
        inference_results = {}
        for model_path, runner in runners.items():
            try:
                # Test ONNX-only mode
                print(f"\n   Testing {model_path} (onnx_only)...")
                start_time = time.time()
                onnx_result = runner.infer(mode="onnx_only")
                onnx_time = time.time() - start_time
                
                # Test auto mode
                print(f"   Testing {model_path} (auto)...")
                start_time = time.time()
                auto_result = runner.infer(mode="auto")
                auto_time = time.time() - start_time
                
                inference_results[model_path] = {
                    "onnx": onnx_result,
                    "auto": auto_result,
                    "onnx_time": onnx_time,
                    "auto_time": auto_time
                }
                
                # Validate results
                onnx_class = onnx_result.get("predicted_class", -1)
                auto_class = auto_result.get("predicted_class", -1)
                onnx_security = onnx_result.get("overall_security", -1)
                auto_security = auto_result.get("overall_security", -1)
                
                print(f"‚úÖ {model_path}: Inference successful")
                print(f"   ONNX: Class={onnx_class}, Security={onnx_security}%, Time={onnx_time:.3f}s")
                print(f"   Auto: Class={auto_class}, Security={auto_security}%, Time={auto_time:.3f}s")
                print(f"   Classification Match: {onnx_class == auto_class}")
                
            except Exception as e:
                print(f"‚ùå {model_path}: Inference failed: {e}")
        
        if inference_results:
            success_count += 1
        
        # Test 6: Security calculation validation
        total_tests += 1
        print(f"\nüß™ Test 6: Security Calculation Validation")
        
        for model_path, results in inference_results.items():
            onnx_security = results["onnx"].get("overall_security", -1)
            auto_security = results["auto"].get("overall_security", -1)
            
            # ONNX-only should have 0% security
            if onnx_security == 0:
                print(f"‚úÖ {model_path}: ONNX security correct (0%)")
            else:
                print(f"‚ùå {model_path}: ONNX security incorrect ({onnx_security}%)")
            
            # Auto should have higher security based on circuits
            if auto_security > 0 and isinstance(auto_security, int):
                print(f"‚úÖ {model_path}: Auto security correct ({auto_security}% - whole number)")
            else:
                print(f"‚ö†Ô∏è  {model_path}: Auto security: {auto_security}%")
        
        success_count += 1
        
        # Test 7: File overwriting validation
        total_tests += 1
        print(f"\nüß™ Test 7: File Overwriting Validation")
        
        for model_path in available_models:
            try:
                model_name = os.path.basename(model_path)
                output_file = f"{model_path}/output.json"
                
                if os.path.exists(output_file):
                    with open(output_file, 'r') as f:
                        output_data = json.load(f)
                    
                    # Check if it's recent (overwritten)
                    timestamp = output_data.get("timestamp", 0)
                    age_minutes = (time.time() - timestamp) / 60
                    
                    if age_minutes < 5:  # Recent output
                        print(f"‚úÖ {model_path}: Output file recently overwritten ({age_minutes:.1f} min ago)")
                    else:
                        print(f"‚ö†Ô∏è  {model_path}: Output file old ({age_minutes:.1f} min ago)")
                else:
                    print(f"‚ùå {model_path}: No output file found")
                    
            except Exception as e:
                print(f"‚ùå {model_path}: Output validation failed: {e}")
                
        success_count += 1
        
        # Test 8: Warning system validation
        total_tests += 1
        print(f"\nüß™ Test 8: Warning System Validation")
        
        # Test warnings for existing circuits
        for model_path in available_models:
            try:
                ezkl_dir = f"{model_path}/ezkl"
                if os.path.exists(ezkl_dir):
                    print(f"‚úÖ {model_path}: EzKL circuits exist - warnings should appear")
                else:
                    print(f"‚ÑπÔ∏è  {model_path}: No EzKL circuits - no warnings expected")
            except Exception as e:
                print(f"‚ùå {model_path}: Warning validation failed: {e}")
        
        success_count += 1
        
        # Final summary
        print(f"\n" + "=" * 60)
        print(f"üìä PRODUCTION VALIDATION SUMMARY")
        print(f"=" * 60)
        print(f"‚úÖ Tests Passed: {success_count}/{total_tests}")
        print(f"üõ°Ô∏è  Security Status:")
        
        for model_path, results in inference_results.items():
            auto_security = results["auto"].get("overall_security", 0)
            onnx_security = results["onnx"].get("overall_security", 0)
            print(f"   {os.path.basename(model_path)}: {auto_security}% security, {onnx_security}% fallback")
        
        if success_count == total_tests:
            print(f"\nüéâ ALL SYSTEMS OPERATIONAL - READY FOR PRODUCTION")
            return True
        else:
            print(f"\n‚ö†Ô∏è  {total_tests - success_count} TESTS FAILED - REVIEW REQUIRED")
            return False
            
    except Exception as e:
        logger.error(f"System validation failed: {e}")
        print(f"\n‚ùå SYSTEM VALIDATION FAILED: {e}")
        return False

def test_logit_differences():
    """Test logit differences between inference methods."""
    print(f"\nüßÆ LOGIT DIFFERENCE VALIDATION")
    print("=" * 60)
    print("‚ÑπÔ∏è  Currently testing: WHOLE MODEL vs CIRCUIT-SIMULATED SLICED INFERENCE")
    print("‚ÑπÔ∏è  Note: Mock EzKL circuit execution with computational differences")
    print("‚ÑπÔ∏è  This measures accuracy differences between circuit vs ONNX computation")
    
    try:
        from runner import Runner
        import numpy as np
        
        models = ["models/doom", "models/net"]
        
        for model_path in models:
            model_name = os.path.basename(model_path)
            print(f"\nüìä {model_name.upper()} Logit Analysis - Circuit vs ONNX Differences")
            print("-" * 60)
            
            try:
                # Check if metadata exists
                metadata_file = f"{model_path}/{model_name}_Runner_Metadata.json"
                if not os.path.exists(metadata_file):
                    print(f"‚ö†Ô∏è  {model_name}: Metadata missing, skipping logit test")
                    continue
                
                runner = Runner(model_path)
                
                # Get whole model inference (single ONNX model - baseline)
                print("üîç Running whole model inference (ONNX baseline)...")
                whole_result = runner.infer(mode="onnx_only")
                whole_logits = whole_result.get("logits", [[]])[0] if whole_result.get("logits") else []
                
                # Get circuit-simulated sliced inference (circuits with computational differences)
                print("üîç Running circuit-simulated sliced inference (auto mode)...")
                sliced_result = runner.infer(mode="auto")
                sliced_logits = sliced_result.get("logits", [[]])[0] if sliced_result.get("logits") else []
                
                if len(whole_logits) > 0 and len(sliced_logits) > 0 and len(whole_logits) == len(sliced_logits):
                    # Calculate logit differences with 12 decimal precision
                    whole_array = np.array(whole_logits, dtype=np.float64)
                    sliced_array = np.array(sliced_logits, dtype=np.float64)
                    
                    abs_diff = np.abs(whole_array - sliced_array)
                    max_abs_error = np.max(abs_diff)
                    mean_abs_error = np.mean(abs_diff)
                    sum_abs_error = np.sum(abs_diff)
                    relative_error = max_abs_error / (np.max(np.abs(whole_array)) + 1e-15)
                    
                    # Classification comparison
                    whole_class = whole_result.get("predicted_class", -1)
                    sliced_class = sliced_result.get("predicted_class", -1)
                    classification_match = whole_class == sliced_class
                    
                    # Security comparison
                    whole_security = whole_result.get("overall_security", 0)
                    sliced_security = sliced_result.get("overall_security", 0)
                    
                    # Execution details
                    execution_results = sliced_result.get("execution_results", [])
                    circuit_count = sum(1 for r in execution_results if r.get("method") == "ezkl_circuit")
                    onnx_count = sum(1 for r in execution_results if r.get("method") == "onnx_slice")
                    
                    print(f"üìè Logit Dimensions: {whole_array.shape}")
                    print(f"üìä Max Absolute Error: {max_abs_error:.12f}")
                    print(f"üìä Mean Absolute Error: {mean_abs_error:.12f}")
                    print(f"üìä Sum Absolute Error: {sum_abs_error:.12f}")
                    print(f"üìä Relative Error: {relative_error:.12f}")
                    print(f"üéØ ONNX Baseline Class: {whole_class} (Security: {whole_security}%)")
                    print(f"üéØ Circuit-Sim Class: {sliced_class} (Security: {sliced_security}%)")
                    print(f"‚úÖ Classification Match: {classification_match}")
                    print(f"‚öôÔ∏è  Execution: {circuit_count} circuits, {onnx_count} ONNX slices")
                    
                    # Expected class ranges for validation
                    expected_ranges = {
                        "doom": (0, 6),  # 7 classes (0-6)
                        "net": (0, 9)    # 10 classes (0-9)  
                    }
                    
                    if model_name in expected_ranges:
                        min_class, max_class = expected_ranges[model_name]
                        whole_in_range = min_class <= whole_class <= max_class
                        sliced_in_range = min_class <= sliced_class <= max_class
                        
                        print(f"üìã Expected range: {min_class}-{max_class}")
                        print(f"‚úÖ ONNX class in range: {whole_in_range}")
                        print(f"‚úÖ Circuit class in range: {sliced_in_range}")
                    
                    # Detailed error assessment with 12 decimals
                    print(f"\nüî¨ DETAILED ERROR ANALYSIS:")
                    if max_abs_error == 0.0:
                        print(f"‚ùå {model_name}: NO COMPUTATIONAL DIFFERENCES DETECTED")
                        print(f"   This suggests fallback is being used instead of circuits")
                    elif max_abs_error < 1e-12:
                        print(f"üéâ {model_name}: VIRTUALLY IDENTICAL (error < 1e-12)")
                    elif max_abs_error < 1e-9:
                        print(f"‚úÖ {model_name}: EXCELLENT CIRCUIT PRECISION (error < 1e-9)")
                    elif max_abs_error < 1e-6:
                        print(f"‚úÖ {model_name}: GOOD CIRCUIT PRECISION (error < 1e-6)")
                    elif max_abs_error < 1e-3:
                        print(f"‚ö†Ô∏è  {model_name}: MODERATE PRECISION LOSS (error < 1e-3)")
                    elif classification_match:
                        print(f"‚ö†Ô∏è  {model_name}: SIGNIFICANT PRECISION LOSS but same classification")
                    else:
                        print(f"‚ùå {model_name}: POOR CIRCUIT PRECISION - different classifications")
                    
                    # Show top 3 largest differences by index
                    sorted_indices = np.argsort(abs_diff)[::-1][:3]
                    print(f"\nüìà Top 3 Largest Differences:")
                    for i, idx in enumerate(sorted_indices):
                        whole_val = whole_array[idx]
                        circuit_val = sliced_array[idx]
                        diff_val = abs_diff[idx]
                        print(f"   {i+1}. Index {idx}: ONNX={whole_val:.12f}, Circuit={circuit_val:.12f}, Diff={diff_val:.12f}")
                        
                    print(f"\nüìù Interpretation: Circuit computational differences vs ONNX baseline")
                    print(f"üìù Error represents precision trade-off for cryptographic verification")
                        
                else:
                    print(f"‚ùå {model_name}: Cannot compare logits - dimension mismatch")
                    print(f"   ONNX: {len(whole_logits)}, Circuit-sim: {len(sliced_logits)}")
                    
            except Exception as e:
                print(f"‚ùå {model_name}: Logit test failed: {e}")
    
    except Exception as e:
        print(f"‚ùå Logit difference validation failed: {e}")

if __name__ == "__main__":
    """Run production validation."""
    
    print("üöÄ STARTING PRODUCTION DEPLOYMENT VALIDATION")
    print("=" * 60)
    
    # Change to src directory for proper imports
    current_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.dirname(current_dir)
    os.chdir(src_dir)
    sys.path.insert(0, src_dir)
    
    try:
        # Run system validation
        system_valid = validate_system()
        
        # Run logit difference validation
        test_logit_differences()
        
        if system_valid:
            print(f"\nüéâ PRODUCTION DEPLOYMENT READY")
            print(f"‚úÖ All systems validated and operational")
            sys.exit(0)
        else:
            print(f"\n‚ö†Ô∏è  PRODUCTION DEPLOYMENT NOT READY")
            print(f"‚ùå Some tests failed - review required")
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå DEPLOYMENT VALIDATION FAILED: {e}")
        sys.exit(1) 